---
title: "Zusatztutorium Mathe A WS19/20"
author: "Anton Hanke, Maximillian Kohnen, Felix Schnabel"
date: "Fragestunde: 27/11/19"
output:
  html_document:
    toc: TRUE
    toc_depth: 3
    toc_float: FALSE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Mathematische Logik
!!MACHT FELIX!!

## Aussagen
!!MACHT FELIX!!

### Implikationen
!!MACHT FELIX!!

## Quantoren
!!MACHT FELIX!!

## Beweise
!!MACHT FELIX!!

# Mengen und algebraische Struckturen
Mengen sind Zusammenfassungen bestimmter, wohlunterscheidbarer Objekte.
Für jedes Objekt ist eine klare zuordnung zur Menge erkentlich

-------------

Mengen sind keine Aussagen!!

-------------


## sonder mengen & Mengen Relationen
- $\emptyset \subset \mathbb{N} \subset \mathbb{Z} \subset \mathbb{Q} \subset \mathbb{R} \subset \mathbb{C}$
- $A \subset B$ !Aussage!
- $A \cap B$
- $A \cup B$
- $A\setminus B\quad \wedge \quad B\setminus A$
- $A \times B = \left\{ (a,\,b): a \in A, b \in B\right\}$

## Abbildungen
$$f: A \rightarrow B$$

- A $=$ Definitionsmenge, von hier bilden wir ab.
- B $=$ Zielmenge, hierdrauf wird abgebildet.
- Bildmenge: $\subset B$ welche sich aus $f(A)$ ergibt.

1. Injektive Abbildung: $\forall i\in B|~~ \# (a \in A) \leq 1 : f(a) \rightarrow i$
2. Surjektive Abbildung: $\forall i \in B|~~\# (a \in A) \geq 1 : f(a) \rightarrow i$
3. Bijektive Abbildung: $\forall i \in B|~~ \# (a \in A) = 1 : f(a) \rightarrow i \qquad$ (1. $\wedge$ 2.)

### Gruppen $(G, \oplus)$

- **Abgeschlossenheit** $$a \in G, b \in G : a \oplus b \in G$$
- **Assoziativität** $$(b\oplus a) \oplus c = a \oplus (b \oplus c)$$
- **Neutrales Element** $D_{0}$ $$\exists e \in G, \forall a \in G : a \oplus e = a$$
- **Inverses Element** $$\forall a \in G, \exists \bar{a} \in G: a \oplus \bar{a} = e$$
- **Kommultativität** (abelsche Gruppe):$$\forall a \in G, \forall b \in G: a \oplus b = b \oplus a$$

### Ringe $(M, \oplus, \otimes)$

1. $(M, \oplus)$ ablesche Gruppe
2. $a \otimes ( b \otimes c) = (a \otimes b) \otimes c$ assoziativität gegeben.
3. Distributiv: $\forall a,b,c \in M : a \otimes (b\oplus c) = a \otimes b \oplus a \otimes c$.

- Kommutativ wenn: $a \otimes b = b \otimes a$
- unitär wenn: $\exists 1 \in M : a \otimes 1 = 1 \otimes a = a$.

### Körper $(K, \oplus, \otimes)$
1. $(K, \oplus)$ is abelsche Gruppe mit $D_0 = 0$.
2. $(K \setminus \{0\}, \otimes)$ abelsche Gruppe mit $D_0 = 1$.
3. Distributivgesetz gilt.

- **Unterschied zu Ringen:** $(M, \otimes)$ keine abelsche Gruppe, kein Inverses!

## Vektorrechnung
Vektoren sind tupel mit $n$ elementen ($n = \dim{V}$).  
Sie erfüllen alle bedingungen eines Körpers und lassen sich
nicht mit sich selbst multiplizieren.

- Linearkombination:
$$\vec{z} = \sum _{ i = 1 } ^ k \mu _i \vec{x} _i \in V$$
Hierbei sind $\mu$ skalare $(\mu \in \mathbb{R})$

- Skalarprodukt:
"Vektor multiplikation".  
$$\mathbb{R}^n \mathbb{R}^n = \mathbb{R}$$
Relevant ist, das beide Vektoren gleiche Dimension haben.  
$$ \vec{v} \cdot \vec{w} = \sum _{i = 1} ^n v_i w_i \in \mathbb{R}$$

- Vektor betrag:
$$|\vec{v}|^2 = \vec{v}\cdot\vec{v}$$
$$\Rightarrow |\vec{v}| = \sqrt{\sum_{i = 1} ^n v_i^2}$$
Ein Vektor lässt sich normieren mit: $\vec{e}_v = \frac{\vec{v}}{|\vec{v}|}$.
In $\mathbb{R}^{2}$ gilt: $\vec{e} = \left(\begin{array}{c}\cos \alpha\\ \sin \alpha \end{array}\right)$

- Winkel zwischen Vektoren:  
  Sind vektoren ortogonal ($\alpha = 90^{circ}$) gilt: $\vec{u}\cdot\vec{v} = 0 \Leftrightarrow \vec{u} \bot \vec{v}$  
  Allgemein berechnet sich der Winkel mit:
  $$\vec{u}\cdot\vec{v} = |\vec{u}||\vec{v}|\cos\theta$$


### Basis eines Vektorraums
Die Basis eines Vektorraums ist die Menge an vektoren,
mit welchen sich über Linearkombination jeder Vektor
im Vektorraum berechnen lässt, sie wird der span des Raums gennant:
$$\forall \vec{v} \in V: \exists \lambda_{1}, \ldots, \lambda_{k} \in \mathbb{R}: \vec{v}=\sum_{i=1}^{n} \lambda_{i} \vec{e}_{i}$$  
Die Vektoren dieser Basis spannen den Vektorraum auf und werden als $\text{span}{V}$ bezeichent, wobei $V :\Leftrightarrow \{\vec{v}_i,\dots, \vec{v}_k\} \in \mathbb{R}$

Drei relevante Basen sind:

1. Kanonische Basis: $\mathbb{R}^{n} \left\{ \vec{e}_{1} = \left( 1,\dots,0 \right), \vec{e}_{i} = (0,\dots, 1,\dots, 0), \vec{e}_{n} = \left( 0,\dots,1 \right)\right\}\quad i = 1, \dots, n$
2. normierte Basis: $\left\{ \vec{v}_{i} \in X \right\}: |\vec{v}_{i}| = 1~~ \forall i = 1, \dots, n$
3. orthogonale Basis: $\left\{ \vec{v}_{i} \in X \right\}: \vec{v}_{i} \cdot \vec{v}_{j} = 0~~\forall i,j = 1, \dots, n$

Alle Vektoren der Basis des Vektorraums müssen linear unabhängig voneinander sein:
$$\sum_{i=1}^{r} \lambda_{1} \vec{v}_{1}+\ldots_{i}+\lambda_{r} \vec{v}_{r}^{2}=\vec{O} \Leftrightarrow \lambda_{i}=0 \quad i=1, \ldots, r$$
Lineare Abbhängigkeit ist gegeben, wenn $\exists \lambda \neq 0$ sodass $\lambda\vec{v}_{1}\cdot\lambda\vec{v}_{2} = \vec{0}$.  

Die Dimension des (aufgespannten) Vektorraums entspricht der
Anzahl an Basis oder Span Vektoren.
$$\dim V = \text{span} \left( V \right)$$



# Komplexe Zahlen und trignometrische Funktionen

## Darstellungen Komplexer Zahlen

### Kartesische Darstellung

### Polarkoordinaten Darstellung

### Euler Darstellung

## Rechenoperationen Komplexer Zahlen

## Trigonometrische Funktione

### Geometrische Interpretation

### Eigenschaften und wichtige Gleichungen

### Wichtige Werte

# Matrizen und Lineare Algebra
Lineare Gleichungssysteme stellen sich wie folgt da:
\[\left\{
\begin{array}{lcll}
    \lambda_{1,\:1}~ x_1 + 
    \lambda_{1,\:\dots }~ x_{\dots} + 
    \lambda_{i,\:1}~ x_i &=& b_1 & \text{Gl.}~ 1 \\
    \lambda_{1,\:\dots}~ x_1 + 
    \lambda_{\dots ,\:\dots}~ x_{\dots} + 
    \lambda_{i,\:\dots }~ x_{i} &=& b_{\dots} & \text{Gl.}~\dots \\
    \lambda_{1,\:j}~ x_1 + 
    \lambda_{\dots,\:j}~ x_{\dots} + 
    \lambda_{i,\:j}~ x_i &=& b_k & \text{Gl.}~ k
\end{array}
\right.\]
Dies lässt sich wie folgt umschreiben: $$Ax = b$$
Dabei sind $x$ und $b$ vectoren.
$A$ ist eine Matrix.
$$ A = \begin{pmatrix}
    \lambda_{1,\:1} & \lambda_{1,\:\dots } & \lambda_{i,\:1} \\
    \lambda_{1,\:\dots} & \lambda_{\dots ,\:\dots} & \lambda_{i,\:\dots } \\
    \lambda_{1,\:j} & \lambda_{\dots,\:j} & \lambda_{i,\:j} \\
\end{pmatrix}$$
Eine Matrix wird durch ihre Dimensionen beschreiben:

- $m$: \# Zeilen
- $n$: \# Spalten

## Matrixrechung
Matrizen haben folgende Eigenschaften:

1. Assoziativ
2. Dissoziativ
3. _nicht_ kommutativ!

### Matrix addition/subtraktion
Matrizen müssen identische Dimensionen haben.
Addition der einzelnen Elemente aufeinander.

### Matrix multiplikation
Kriterium: innere Dimensionen gleich.
\[\underset{m\times n}{A} \times \underset{n \times p}{B} = \underset{m \times p}{C}\]
An sich ergibt sich die Ergebnismatrix aus Skalarprodukten der
Zeilen und Spalten der Inputmatrizen.
\[
\begin{pmatrix}
    &&\\
    i_1&i_{C}&i_k\\
    &&\\
\end{pmatrix}
\begin{pmatrix}
    &j_i&\\
    &j_{C}&\\
    &j_k&\\
\end{pmatrix}
\begin{pmatrix}
    &&\\
    &C_{ij}&\\
    &&\\
\end{pmatrix}
\Longleftrightarrow
C_{i,\: j} = \sum_{k = 1}^{n} a_{ik} \cdot b_{jk}
\]

Das neutrale Element der Matrix multiplikation ist die Identitätsmatrix,
eine Diagonalmatrix, mit der Determinante 1:
$$I=
\begin{pmatrix}
    1 & 0 & 0\\
    0 & 1 & 0\\
    0 & 0 & 1\\
\end{pmatrix}
$$

#### Linearkombinationen für Berechnungen:
1. Spalten:  
    $$j_C = x_B^{\: j}\lambda_A + y_B^{\: j} \theta_A + z_B^{\: j} \mu_A$$
    Die Spalte $j$ von $C$ ergibt sich aus der Vektorsumme der Spalten von
    $A$ multipliziert mit den Elementen in der $j$ten Spalte von $B$.
2. Zeilen:
   $$i_C = x_A^{\: i} \lambda_B + y_A^{\: i} \theta_B + z_A^{\: i} \mu_B$$
   Die Zeile $i$ von $C$ ergibt sich aus der Vektersumme der Zeilen von
   $B$ multipliziert mit den Elementen in der $i$ten Zeile von $A$.

### Matrix transposition
"Rotation einer matrix":  
\[ \underset{m \times n}{A} =
\begin{pmatrix}
    a_{1,1} & \cdots & a_{1,n} \\
    \vdots  &        & \vdots  \\
    a_{m,1} & \cdots & a_{m,n} \\
\end{pmatrix} \longrightarrow
\underset{m \times n }{A}^{T} =
\begin{pmatrix}
    a_{1,1} & \cdots & a_{m,1} \\
    \vdots  &        & \vdots  \\
    a_{1,n} & \cdots & a_{m,n} \\
\end{pmatrix}
\]
Spiegelung um die Diagonale.
```{r, echo = F}
matrix(seq(1,9),nrow = 3)
t(matrix(seq(1,9),nrow = 3))
```
Wenn gilt: $A = A^T$ so ist die Matrix Spiegelsymmetrisch.  
Diagonalmatrizen immer Spiegelsymmetrisch.

### Matrix inverse
Die Inverse Matrix ist das Inverse Element eines Elements in dem Körper
der Matrix Multiplikation.  
Es gilt: $A^{-1} A = AA^{-1} = I$


### Matrix Diagonalisierung und determinanten
Durch Diagonalisierung (alle Elemente der Matrix _über/unter_ Diagonale $=0$)
lassen sich die **Pivot Elemente** (Elemente auf Diagonale) bestimmen.  
Generel:
$$EA = A'$$
Dabei $E=$ Eliminationsmatrix.
Die Eliminationsmatrix die Benötigt wird um eine Matrix
vollständig in eine Upper Diagnalmatrix zu überführen ist
die lower Diagonalmatrix der Matrix A.  
$$\underbrace{E'\underbrace{EA}_{A'}}_{A''}$$
Somit:
$$\underbrace{E''}_{\text{under triangel}}A = \overbrace{A''}^{\text{Upper triangel}}$$
Aus den Diagonalmatrizen kann man die **Pivot Elemente** ablesen.
$$
\begin{pmatrix}
    \fbox{x} & x & x \\
    x   & \fbox{x} & x\\
    x   & x & \fbox{x}\\
\end{pmatrix}
$$

<center>
    $\blacktriangleright$ Beachte Multiplikationsreihenfolge, nicht kummutativ $\blacktriangleleft$  
</center>
Die Determinante einer matrix:
$$\det{A} = \prod \text{Pivot Elemente}$$
In einer Matrix mit $\det{A}\neq 0$ gibt es entweder 0 oder $\infty$ viele
Lösungen für Gleichungssysteme.
Die Matrix ist Singulär und hat kein Inverses.

### Spalten und Nullraum

## Eliminationsverfahren

### Matrix erweiterung oder auch Gauß Verfahren

### Matrixform

## Lösbarkeit

